
// OpenAI chat endpoint, for having conversations using "gpt-xxx" models.
.arguments
   prompt:string
   type:string
   recaptcha_response:string
   max:int
.description:OpenAI AI search endpoint returning only references to matching training snippets
.type:public

// Invoking slot containing commonalities for all endpoints.
insert-before:x:../*/signal/*/.callback
   get-nodes:x:@.arguments/*
signal:magic.ai.endpoint-common
   .callback

      // Invoking slot responsible for returning URLs to us.
      add:x:+
         get-nodes:x:@.arguments/*
      signal:magic.ai.search

      // Returning results of above invocation to caller.
      add:x:./*/return-nodes/*/snippets
         get-nodes:x:@signal/*/snippets/*
      add:x:./*/return-nodes
         get-nodes:x:@signal/*/db_time
      return-nodes
         snippets

/*
 * Applying some HTTP caching to avoid invoking OpenAI again with
 * the same question before some minimum amount of time has passed.
 *
 * Notice, we can apply more "aggressive" caching here than in our
 * chat endpoints, since the same query will mostly always return
 * the same result, since there is no room for "creativity" here.
 */
response.headers.set
   Cache-Control:max-age=300

// Returning result of worker slot to caller.
return-nodes:x:@signal/*
