
/*
 * Slot for commonalities between different endpoint responsible for 
 * handling authentication, reCAPTCHA, opening database connection,
 * and all other commonalities between the different endpoints.
 */
slots.create:magic.ai.endpoint-common

   // Sanity checking invocation.
   validators.mandatory:x:@.arguments/*/type
   validators.mandatory:x:@.arguments/*/prompt
   validators.string:x:@.arguments/*/prompt
      min:1

   // Trimming prompt.
   set-value:x:@.arguments/*/prompt
      strings.trim:x:@.arguments/*/prompt

   // Buffer for model settings.
   .model

   // Connecting to database to retrieve model settings.
   data.connect:[generic|magic]

      // Reading settings for type.
      data.read
         table:ml_types
         columns
            model
            max_tokens
            max_context_tokens
            max_request_tokens
            temperature
            recaptcha
            auth
            supervised
            cached
            prefix
            use_embeddings
            threshold
            vector_model
            contact_us
            lead_email
            api_key
            webhook_incoming
            webhook_outgoing
            webhook_incoming_url
            webhook_outgoing_url
            system_message
            initial_questionnaire
         where
            and
               type.eq:x:@.arguments/*/type

      // Verifying type exists.
      if
         not-exists:x:@data.read/*
         .lambda

            // Oops, no such type, trying to see if default type exists.
            data.read
               table:ml_types
               columns
                  model
                  max_tokens
                  max_context_tokens
                  max_request_tokens
                  temperature
                  recaptcha
                  auth
                  supervised
                  cached
                  prefix
                  use_embeddings
                  threshold
                  vector_model
                  contact_us
                  lead_email
                  api_key
                  webhook_incoming
                  webhook_outgoing
                  webhook_incoming_url
                  webhook_outgoing_url
                  system_message
                  initial_questionnaire
               where
                  and
                     type.eq:default
            if
               not-exists:x:@data.read/*
               .lambda

                  // Default type doesn't exist, nothing to do here.
                  throw:No such type, and no default type was found
                     status:int:400
                     public:bool:true

            // Resorting to default type.
            add:x:@.model
               get-nodes:x:@data.read/*/*
            set-value:x:@.arguments/*/type
               .:default

      else

         // Model exists, populating above buffer.
         add:x:@.model
            get-nodes:x:@data.read/*/*

   /*
    * Checking if model requires authentication and authorisation,
    * unless caller explicitly informs us he wants to skip authentication.
    */
   if
      or
         not-exists:x:@.arguments/*/skip-auth
         eq:x:@.arguments/*/skip-auth
            .:bool:false
      .lambda

         // Caller wants default authentication logic, first checking if model requires authentication.
         if
            and
               not-null:x:@.model/*/auth
               neq:x:@.model/*/auth
                  .:
            .lambda

               // Making sure user is authorised to using type.
               auth.ticket.verify:x:@.model/*/auth

         // Checking if model requires reCAPTCHA.
         if
            and
               not
                  auth.ticket.in-role:root
               mt
                  convert:x:@.model/*/recaptcha
                     type:decimal
                  .:decimal:0
            .lambda

               // Verifying reCAPTCHA was supplied.
               if
                  or
                     not-exists:x:@.arguments/*/recaptcha_response
                     null:x:@.arguments/*/recaptcha_response
                  .lambda

                     // Endpoint invoked without reCAPTCHA, making sure we abort invocation.
                     response.status.set:499
                     return
                        error:No reCAPTCHA supplied

               // Retrieving reCAPTCHA site key.
               .key
               set-value:x:@.key
                  config.get:"magic:auth:recaptcha:key"

               // Retrieving reCAPTCHA secret.
               .secret
               set-value:x:@.secret
                  config.get:"magic:auth:recaptcha:secret"

               // Validating reCAPTCHA invocation confirms request originated from a human.
               convert:x:@.model/*/recaptcha
                  type:decimal
               validators.recaptcha:x:@.arguments/*/recaptcha_response
                  min:x:@convert
                  site-key:x:@.key
                  secret:x:@.secret

   // Checking if we've configured integrations for incoming messages.
   .incoming
   set-value:x:@.incoming
      get-first-value
         get-value:x:@.model/*/webhook_incoming
         config.get:"magic:openai:integrations:incoming:slot"
   if
      and
         not-null:x:@.incoming
         neq:x:@.incoming
            .:
      .lambda

         // Invoking integration slot.
         .exe

            // Retrieving URL to invoke.
            .hook-url
            set-value:x:@.hook-url
               get-first-value
                  get-value:x:@.model/*/webhook_incoming_url
                  config.get:"magic:openai:integrations:incoming:url"

            // Invoking slot.
            unwrap:x:./*/signal/*/url
            signal:x:@.incoming
               url:x:@.hook-url

         // Parametrizing invocation to integration slot.
         if
            and
               exists:x:@.arguments/*/to
               exists:x:@.arguments/*/from
               not-null:x:@.arguments/*/to
               not-null:x:@.arguments/*/from
               strings.contains:x:@.arguments/*/to
                  .:":"
               strings.contains:x:@.arguments/*/from
                  .:":"
            .lambda

               // We have a channel to accommodate for.
               .channel
               .to
               .from
               strings.split:x:@.arguments/*/to
                  .:":"
               set-value:x:@.channel
                  get-value:x:@strings.split/0
               set-value:x:@.to
                  get-value:x:@strings.split/1
               strings.split:x:@.arguments/*/from
                  .:":"
               set-value:x:@.from
                  get-value:x:@strings.split/1
               add:x:@.exe/*/signal
                  get-nodes:x:@.arguments/*/prompt
                  get-nodes:x:@.arguments/*/session
               unwrap:x:+/*/*
               add:x:@.exe/*/signal
                  .
                     to:x:@.to
                     from:x:@.from
                     channel:x:@.channel

         else

            // No channel
            add:x:@.exe/*/signal
               get-nodes:x:@.arguments/*/to
               get-nodes:x:@.arguments/*/from
               get-nodes:x:@.arguments/*/prompt
               get-nodes:x:@.arguments/*/session

         // Invoking callback.
         eval:x:@.exe

   // Doing some common conversions.
   set-value:x:@.model/*/threshold
      convert:x:@.model/*/threshold
         type:double
   set-value:x:@.model/*/max_tokens
      convert:x:@.model/*/max_tokens
         type:int
   set-value:x:@.model/*/max_context_tokens
      convert:x:@.model/*/max_context_tokens
         type:int
   set-value:x:@.model/*/max_request_tokens
      convert:x:@.model/*/max_request_tokens
         type:int

   // Making sure se set max size of model
   switch:x:@.model/*/model

      case:gpt-3.5-turbo-16k
      case:gpt-3.5-turbo-16k-0613

         add:x:@.model
            .
               model_size:int:16384

      case:text-davinci-003
      case:gpt-3.5-turbo
      case:gpt-3.5-turbo-0301
      case:gpt-3.5-turbo-0314
      case:gpt-3.5-turbo-0613
      case:text-davinci-002

         add:x:@.model
            .
               model_size:int:4096

      case:code-davinci-002

         add:x:@.model
            .
               model_size:int:8000

      case:gpt-4
      case:gpt-4-0314

         add:x:@.model
            .
               model_size:int:8192

      case:gpt-4-32k
      case:gpt-4-32k-0314

         add:x:@.model
            .
               model_size:int:32768

      default

         add:x:@.model
            .
               model_size:int:2049

   // Adding defaults in case model has not been configured with max_request_tokens
   if
      or
         null:x:@.model/*/max_context_tokens
         null:x:@.model/*/max_request_tokens
      .lambda

         /*
          * Defaulting [max_request_tokens] and [max_context_tokens] according
          * to what model we're using.
          */
         switch:x:@.model/*/model

            case:text-davinci-003
            case:gpt-3.5-turbo
            case:gpt-3.5-turbo-0301
            case:text-davinci-002

               math.divide
                  math.subtract:int:4096
                     get-value:x:@.model/*/max_tokens
                  .:int:2
               set-value:x:@.model/*/max_context_tokens
                  get-value:x:@math.divide
               set-value:x:@.model/*/max_request_tokens
                  get-value:x:@math.divide

            case:code-davinci-002

               math.divide
                  math.subtract:int:8000
                     get-value:x:@.model/*/max_tokens
                  .:int:2
               set-value:x:@.model/*/max_context_tokens
                  get-value:x:@math.divide
               set-value:x:@.model/*/max_request_tokens
                  get-value:x:@math.divide

            case:gpt-4
            case:gpt-4-0314

               math.divide
                  math.subtract:int:8192
                     get-value:x:@.model/*/max_tokens
                  .:int:2
               set-value:x:@.model/*/max_context_tokens
                  get-value:x:@math.divide
               set-value:x:@.model/*/max_request_tokens
                  get-value:x:@math.divide

            case:gpt-4-32k
            case:gpt-4-32k-0314

               math.divide
                  math.subtract:int:32768
                     get-value:x:@.model/*/max_tokens
                  .:int:2
               set-value:x:@.model/*/max_context_tokens
                  get-value:x:@math.divide
               set-value:x:@.model/*/max_request_tokens
                  get-value:x:@math.divide

            default

               math.divide
                  math.subtract:int:2049
                     get-value:x:@.model/*/max_tokens
                  .:int:2
               set-value:x:@.model/*/max_context_tokens
                  get-value:x:@math.divide
               set-value:x:@.model/*/max_request_tokens
                  get-value:x:@math.divide

   // Making sure prompt is not larger than [max_request_tokens].
   if
      mt
         openai.tokenize:x:@.arguments/*/prompt
         convert:x:@.model/*/max_request_tokens
            type:int
      .lambda

         // Oops, more prompt than model allows for.
         throw:Your request is longer than what this type is configured to allow for
            public:bool:true
            status:int:400

   // Invoking callback provided by caller.
   add:x:./*/invoke
      get-nodes:x:@.arguments/*
      get-nodes:x:@.model/*
   remove-nodes:x:./*/invoke/*/.callback
   invoke:x:@.arguments/*/.callback

   // Returning result of invocation to caller.
   return-nodes:x:@invoke/*
