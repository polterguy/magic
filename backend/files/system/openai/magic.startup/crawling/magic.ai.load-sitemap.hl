
/*
 * Loads sitemap according to given [sitemap] collection,
 * [allow] collection, and [disallow] collection, and
 * returns [max] number of links to caller.
 *
 * Optionally supply [headers] collection being HTTP headers to
 * use when retrieving sitemap(s).
 */
slots.create:magic.ai.load-sitemap

   // Sanity checking invocation.
   validators.mandatory:x:@.arguments/*/sitemap/*/[0,1]
   validators.integer:x:@.arguments/*/max
      min:1
      max:10000

   // Defaulting [max] to 25 if not specified.
   validators.default:x:@.arguments
      max:int:25

   // Adding [headers] argument unless already specified.
   if
      not-exists:x:@.arguments/*/headers
      .lambda

         // Adding [headers] to [.arguments] such that we can create default HTTP headers further down.
         add:x:@.arguments
            .
               headers

   // Adding default headers unless they're already specified.
   validators.default:x:@.arguments/*/headers
      User-Agent:AINIRO-Crawler 2.0
      Accept-Encoding:identity
      Accept:text/xml

   // Buffer used for URLs to return.
   .urls

   // True if site contains a sitemap.
   .has-sitemap:bool:false

   // Making sure we catch exceptions.
   try

      /*
       * Iterating through each sitemap specified by caller.
       *
       * Notice, we only iterate through a maximum of 25 sitemaps to
       * avoid exhausting the cloudlet.
       */
      for-each:x:@.arguments/*/sitemap/*/[0,25]

         // Providing feedback to client.
         strings.concat
            .:"Retrieving sitemap "
            get-value:x:@.dp/#
         unwrap:x:+/**
         sockets.signal:x:@.arguments/*/feedback-channel
            args
               message:x:@strings.concat
               type:info
         sleep:100

         // To avoid having cloudlet run out of memory we apply a hard stop at 10,000 URLs.
         if
            lt
               get-count:x:@.urls/*
               .:int:10000
            .lambda

               // Keeping a reference around for the domain.
               .domain
               strings.split:x:@.dp/#
                  .:/
               set-value:x:@.domain
                  strings.concat
                     get-value:x:@strings.split/0
                     .://
                     get-value:x:@strings.split/1

               /*
                * Retrieving sitemap, making sure we parametrize invocation
                * with [headers] collection.
                */
               add:x:./*/http.get
                  get-nodes:x:@.arguments/*/headers
               http.get:x:@.dp/#
                  timeout:60

               // Checking if above invocation succeeded.
               if
                  and
                     not
                        mte:x:@http.get
                           .:int:200
                     not
                        lt:x:@http.get
                           .:int:300
                  .lambda

                     // Sitemap invocation did not return success status code.
                     log.error:Sitemap invocation did not return success status
                        url:x:@.dp/#
                        status:x:@http.get
                     throw:Could not retrieve sitemap

               // Verifying request returned text/xml MIME type or application/xml MIME type.
               if
                  not
                     or
                        strings.starts-with:x:@http.get/*/headers/*/Content-Type
                           .:application/xml
                        strings.starts-with:x:@http.get/*/headers/*/content-type
                           .:application/xml
                        strings.starts-with:x:@http.get/*/headers/*/Content-Type
                           .:text/xml
                        strings.starts-with:x:@http.get/*/headers/*/content-type
                           .:text/xml
                  .lambda

                     // Bogus sitemap file, not XML.
                     throw:Sitemap was not XML

               else

                  // Making sure we return the fact that site has at least one sitemap.
                  set-value:x:@.has-sitemap
                     .:bool:true

                  // Some sitemaps contains bogus space characters, removing these to be sure.
                  strings.trim:x:@http.get/*/content

                  // Transforming sitemap from XML to lambda.
                  xml2lambda:x:@strings.trim

                  // Iterating through each URL in currently iterated sitemap file.
                  for-each:x:@xml2lambda/*/urlset/*/url/*/loc/*/\#text

                     // To avoid having cloudlet run out of memory we apply a hard stop at 10,000 URLs.
                     if
                        lt
                           get-count:x:@.urls/*
                           .:int:10000
                        .lambda

                           // URL we're currently adding.
                           .url
                           set-value:x:@.url
                              get-value:x:@.dp/#
                           if
                              and
                                 not
                                    strings.starts-with:x:@.dp/#
                                       .:"http://"
                                 not
                                    strings.starts-with:x:@.dp/#
                                       .:"https://"
                              .lambda

                                 // Need to prepend URL with domain.
                                 set-value:x:@.url
                                    strings.concat
                                       get-value:x:@.domain
                                       get-value:x:@.dp/#

                           // Avoiding duplicates.
                           if
                              not-exists:x:@.urls/*/={@.url}
                              .lambda

                                 // URL does not exist from before in above [.urls] collection.
                                 unwrap:x:+/*/*
                                 add:x:@.urls
                                    .
                                       .:x:@.url

                  // Iterating through each CDATA URL in currently primary sitemap file.
                  for-each:x:@xml2lambda/*/urlset/*/url/*/loc/*/\#cdata-section

                     // To avoid having cloudlet run out of memory we apply a hard stop at 10,000 URLs.
                     if
                        lt
                           get-count:x:@.urls/*
                           .:int:10000
                        .lambda

                           // URL we're currently adding.
                           .url
                           set-value:x:@.url
                              get-value:x:@.dp/#
                           if
                              and
                                 not
                                    strings.starts-with:x:@.dp/#
                                       .:"http://"
                                 not
                                    strings.starts-with:x:@.dp/#
                                       .:"https://"
                              .lambda

                                 // Need to prepend URL with domain.
                                 set-value:x:@.url
                                    strings.concat
                                       get-value:x:@.domain
                                       get-value:x:@.dp/#

                           // Avoiding duplicates.
                           if
                              not-exists:x:@.urls/*/={@.url}
                              .lambda

                                 // URL does not exist from before in above [.urls] collection.
                                 unwrap:x:+/*/*
                                 add:x:@.urls
                                    .
                                       .:x:@.url

                  /*
                   * Iterating through each CDATA URL referenced in main sitemap.
                   *
                   * Notice, we only iterate through the first 25 sitemap references to
                   * avoid exhausting cloudlet.
                   */
                  for-each:x:@xml2lambda/*/sitemapindex/*/sitemap/*/[0,25]/loc/*/\#cdata-section

                     // To avoid having cloudlet run out of memory we apply a hard stop at 10,000 URLs.
                     if
                        lt
                           get-count:x:@.urls/*
                           .:int:10000
                        .lambda

                           // Recursively invoking self.
                           add:x:./*/signal
                              get-nodes:x:@.arguments/*/headers
                           unwrap:x:+/**
                           signal:magic.ai.load-sitemap
                              max:int:10000
                              feedback-channel:x:@.arguments/*/feedback-channel
                              filter-on-url:x:@.arguments/*/filter-on-url
                              sitemap
                                 .:x:@.dp/#

                           // Adding URLs to above [.urls] collection.
                           add:x:@.urls
                              get-nodes:x:@signal/*/urls/*

                           // If above invocation returned no sitemap, and we've got zero URLs, we set [.has-sitemap] to false.
                           if
                              and
                                 eq:x:@signal/*/has-sitemap
                                    .:bool:false
                                 eq
                                    get-count:x:@.urls/*
                                    .:int:0
                              .lambda
                                 set-value:x:@.has-sitemap
                                    .:bool:false

                  /*
                   * Iterating through each text URL referenced in main sitemap.
                   *
                   * Notice, we only iterate through the first 25 sitemap references to
                   * avoid exhausting cloudlet.
                   */
                  for-each:x:@xml2lambda/*/sitemapindex/*/sitemap/*/[0,25]/loc/*/\#text

                     // To avoid having cloudlet run out of memory we apply a hard stop at 10,000 URLs.
                     if
                        lt
                           get-count:x:@.urls/*
                           .:int:10000
                        .lambda

                           // Recursively invoking self.
                           unwrap:x:+/**
                           signal:magic.ai.load-sitemap
                              max:int:10000
                              feedback-channel:x:@.arguments/*/feedback-channel
                              filter-on-url:x:@.arguments/*/filter-on-url
                              sitemap
                                 .:x:@.dp/#

                           // Adding URLs to above [.urls] collection.
                           add:x:@.urls
                              get-nodes:x:@signal/*/urls/*

                           // If above invocation returned no sitemap, and we've got zero URLs, we set [.has-sitemap] to false.
                           if
                              and
                                 eq:x:@signal/*/has-sitemap
                                    .:bool:false
                                 eq
                                    get-count:x:@.urls/*
                                    .:int:0
                              .lambda
                                 set-value:x:@.has-sitemap
                                    .:bool:false

      // Total URLs in sitemap(s).
      .total
      set-value:x:@.total
         get-count:x:@.urls/*

      /*
       * Sorting URLs by length, which typically makes sure more
       * important URLs are prioritised.
       */
      sort:x:@.urls/*
         if
            lt
               strings.length:x:@.lhs/#
               strings.length:x:@.rhs/#
            .lambda
               set-value:x:@.result
                  .:int:-1
         else-if
            mt
               strings.length:x:@.lhs/#
               strings.length:x:@.rhs/#
            .lambda
               set-value:x:@.result
                  .:int:1
         else
            set-value:x:@.result
               .:int:0

      /*
       * Helper lambda to determine if [url] is matching [pattern].
       *
       * Not complete implementation of specification, but supports
       * $ and * character at the end of the line, used as wildcard
       * and end of string.
       */
      .match

         // Wildcard character.
         if
            strings.ends-with:x:@.arguments/*/pattern
               .:*
            .lambda
               strings.trim-end:x:@.arguments/*/pattern
                  .:*
               strings.starts-with:x:@.arguments/*/url
                  get-value:x:@strings.trim-end
               return-value:x:@strings.starts-with

         // $ sign, end of line.
         if
            strings.ends-with:x:@.arguments/*/pattern
               .:$
            .lambda
               strings.trim-end:x:@.arguments/*/pattern
                  .:$
               eq:x:@.arguments/*/url
                  get-value:x:@strings.trim-end
               return-value:x:@eq

         // Default logic.
         strings.starts-with:x:@.arguments/*/url
            get-value:x:@.arguments/*/pattern
         return-value:x:@strings.starts-with

      // Making sure we don't return more than [max] URLs.
      .return
      .ignored:int:0
      while
         and
            mt:x:@.arguments/*/max
               .:int:0
            exists:x:@sort/0
         .lambda

            // Making sure URL is not disallowed.
            .allowed:bool:true
            for-each:x:@.arguments/*/disallow/*

               // Checking if currently iterated URL starts with disallow part.
               unwrap:x:+/*
               invoke:x:@.match
                  url:x:@sort/0
                  pattern:x:@.dp/#
               if
                  get-value:x:@invoke
                  .lambda

                     // Disallowed URL.
                     set-value:x:@.allowed
                        .:bool:false

                     // Checking if URL is explicitly allowed again.
                     for-each:x:@.arguments/*/allow/*

                        // Checking if currently iterated URL is explicitly allowed.
                        unwrap:x:+/*
                        invoke:x:@.match
                           url:x:@sort/0
                           pattern:x:@.dp/#
                        if
                           get-value:x:@invoke
                           .lambda

                              // Explicitly allowed URL.
                              set-value:x:@.allowed
                                 .:bool:true

            // Verifying URL starts with base URL.
            if
               and
                  get-value:x:@.arguments/*/filter-on-url
                  exists:x:@.arguments/*/url
                  not-null:x:@.arguments/*/url
                  neq:x:@.arguments/*/url
                     .:
                  not
                     strings.starts-with:x:@sort/0
                        get-value:x:@.arguments/*/url
               .lambda

                  // URL does not match base URL of scraping invocation.
                  set-value:x:@.allowed
                     .:bool:false

            // Verifying URL is allowed.
            if
               eq:x:@.allowed
                  .:bool:true
               .lambda

                  // URL is allowed.
                  add:x:@.return
                     get-nodes:x:@sort/0

                  // Decrementing URL count.
                  math.decrement:x:@.arguments/*/max

            else

               // Informing user that URL is disallowed.
               math.increment:x:@.ignored

            // Removing currently iterated URL.
            remove-nodes:x:@sort/0

      // Checking if we've got URLs we should not import.
      if
         neq:x:@.ignored
            .:int:0
         .lambda

            // Notifying user we've got ignored URLs.
            strings.concat
               get-value:x:@.ignored
               .:" URLs disallowed by robots.txt or not matching base URL"
            unwrap:x:+/**
            sockets.signal:x:@.arguments/*/feedback-channel
               args
                  message:x:@strings.concat
                  type:warning
            sleep:100

      // Returning URLs to caller.
      add:x:./*/return-nodes/*/urls
         get-nodes:x:@.return/*
      unwrap:x:+/*
      return-nodes
         has-sitemap:x:@.has-sitemap
         total:x:@.total
         urls

   .catch

      // Providing feedback to client.
      strings.concat
         .:"Something went wrong as we tried to load sitemap, error was; '"
         get-value:x:@.arguments/*/message
         .:"'"
      unwrap:x:+/**
      sockets.signal:x:@.arguments/@.arguments/*/feedback-channel
         roles:root
         args
            message:x:@strings.concat
            type:warning
      sleep:100

      // Returning "no URLs" to caller
      return-nodes
         has-sitemap:bool:false
