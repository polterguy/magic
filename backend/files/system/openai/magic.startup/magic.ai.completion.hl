
/*
 * Completion type of slot for non-GPT type of models.
 */
slots.create:magic.ai.completion

   /*
    * Checking if model is configured to return cached requests.
    *
    * Notice, the type needs to have turned on cache, and we need to have a
    * historical request that's cached matching the prompt and the type,
    * and caller must not have asked for references for the cache to kick in.
    */
   if
      and
         exists:x:@.arguments/*/cached
         not-null:x:@.arguments/*/cached
         eq
            convert:x:@.arguments/*/cached
               type:int
            .:int:1
         or
            not-exists:x:@.arguments/*/references
            not
               get-value:x:@.arguments/*/references
      .lambda

         // Checking if we can find a cached request matching prompt and type.
         data.read
            table:ml_requests
            columns
               completion
               cached
            where
               and
                  prompt.eq:x:@.arguments/*/prompt
                  type.eq:x:@.arguments/*/type
            limit:1
            order:created
            direction:desc

         // Checking if we could find a matching request in cache.
         if
            and
               exists:x:@data.read/*/*
               eq
                  convert:x:@data.read/*/*/cached
                     type:int
                  .:int:1
            .lambda

               // Returning cached completion.
               unwrap:x:+/*
               return
                  result:x:@data.read/*/*/completion
                  finish_reason:cached

   // Result we're returning to caller.
   .result

   // Prompt that might be changed if model contains a prefix, and/or we're using embeddings.
   .prompt

   /*
    * Checking if model is configured to use embeddings.
    *
    * If model is configured to using embeddings, we find relevant training snippets from
    * training data and use as "CONTEXT". If not, it might be either a fine-tuned model,
    * or a "mint" model, where mint implies no context or customisation having been done,
    * which is true if user just wants the default AI model without any customisations.
    */
   if
      eq
         convert:x:@.arguments/*/use_embeddings
            type:int
         .:int:1
      .lambda

         // Retrieving relevant snippets.
         unwrap:x:+/*
         signal:magic.ai.get-context
            type:x:@.arguments/*/type
            prompt:x:@.arguments/*/prompt
            threshold:x:@.arguments/*/threshold
            max_tokens:x:@.arguments/*/max_context_tokens
            vector_model:x:@.arguments/*/vector_model
         add:x:@.result
            get-nodes:x:@signal/*/db_time

         // Correctly applying prefix.
         if
            and
               strings.contains:x:@.arguments/*/prefix
                  .:[CONTEXT]
               strings.contains:x:@.arguments/*/prefix
                  .:[QUESTION]
               strings.contains:x:@.arguments/*/prefix
                  .:"ANSWER:"
            .lambda

               // Structured prefix.
               set-value:x:@.prompt
                  strings.replace:x:@.arguments/*/prefix
                     .:[QUESTION]
                     get-value:x:@.arguments/*/prompt
               set-value:x:@.prompt
                  strings.replace:x:@.prompt
                     .:[CONTEXT]
                     get-value:x:@signal/*/context

         else

            // Old style prefix.
            set-value:x:@.prompt
               strings.concat
                  get-value:x:@.arguments/*/prefix
                  .:"\r\nQUESTION: "
                  get-value:x:@.arguments/*/prompt
                  .:"\r\nCONTEXT: \r\n"
                  get-value:x:@signal/*/context
                  .:"\r\nANSWER: "

         // Checking if caller wants references.
         if
            and
               exists:x:@.arguments/*/references
               not-null:x:@.arguments/*/references
               get-value:x:@.arguments/*/references
            .lambda
               add:x:@.result
                  .
                     references
               add:x:@.result/*/references
                  get-nodes:x:@signal/*/snippets/*

   else

      /*
       * Not using embeddings, either a mint model or a fine-tuned model.
       *
       * Checking if model is fine-tuned, which is true if model name contains
       * a colon (:), at which point we apply "->" and " END" to help model roundtrip.
       */
      if
         strings.contains:x:@.arguments/*/model
            .:":"
         .lambda

            // Fine-tuned model, making sure we apply STOP characters to prompt and invocation.
            set-value:x:@.prompt
               strings.concat
                  get-value:x:@.arguments/*/prompt
                  .:" ->"
            add:x:../*/http.post/*/payload
               .
                  stop:" END"

      else

         // "Mint" model - Implying default model without any customisations.
         set-value:x:@.prompt
            get-value:x:@.arguments/*/prompt

   // Retrieving token.
   .token
   set-value:x:@.token
      strings.concat
         .:"Bearer "
         config.get:"magic:openai:key"

   // Invoking OpenAI, now with a decorated prompt.
   http.post:"https://api.openai.com/v1/completions"
      headers
         Authorization:x:@.token
         Content-Type:application/json
      payload
         prompt:x:@.prompt
         model:x:@.arguments/*/model
         max_tokens:x:@.arguments/*/max_tokens
         temperature:x:@.arguments/*/temperature
      convert:true

   // Sanity checking above invocation.
   if
      not
         and
            mte:x:@http.post
               .:int:200
            lt:x:@http.post
               .:int:300
      .lambda

         // Oops, error - Logging error and returning status 500 to caller.
         lambda2hyper:x:@http.post
         log.error:Something went wrong while invoking OpenAI
            message:x:@http.post/*/content/*/error/*/message
            status:x:@http.post
            error:x:@lambda2hyper
         response.status.set:x:@http.post
         unwrap:x:+/*
         return
            error:bool:true
            result:x:@http.post/*/content/*/error/*/message
   else

      // Success! Logging as such!
      log.info:Invoking OpenAI was a success

   // Making sure we trim response before adding it to [.result].
   strings.trim:x:@http.post/*/content/*/choices/0/*/text
   unwrap:x:+/*/*
   add:x:@.result
      .
         result:x:@strings.trim

   // Making sure we return finish reason to caller.
   get-first-value
      get-value:x:@http.post/*/content/*/choices/0/*/finish_reason
      .:unknown
   unwrap:x:+/*/*
   add:x:@.result
      .
         finish_reason:x:@get-first-value

   // Checking if type is 'supervised', at which point we store prompt and completion.
   if
      and
         not
            get-value:x:@.result/*/error
         not
            exists:x:@data.read/*/*
         mt
            convert:x:@.arguments/*/supervised
               type:int
            .:int:0
      .lambda

         // Storing prompt and completion in ml_requests table.
         data.create
            table:ml_requests
            values
               type:x:@.arguments/*/type
               prompt:x:@.arguments/*/prompt
               completion:x:@.result/*/result
               finish_reason:x:@.result/*/finish_reason

   /*
    * Applying some HTTP caching to avoid invoking OpenAI again with
    * the same question before some minimum amount of time has passed.
    */
   response.headers.set
      Cache-Control:max-age=30

   // Returning results returned from invocation above to caller.
   add:x:+
      get-nodes:x:@.result/*/result
      get-nodes:x:@.result/*/finish_reason
      get-nodes:x:@.result/*/references
      get-nodes:x:@.result/*/db_time
   return
