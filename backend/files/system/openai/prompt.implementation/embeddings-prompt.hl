
// Invokes OpenAI to find a vector from the specified prompt.
.token
set-value:x:@.token
   strings.concat
      .:"Bearer "
      config.get:"magic:openai:key"

// Associating request with user, if applicable.
auth.ticket.get
if
   not-null:x:@auth.ticket.get
   .lambda

      // Associating request with username.
      unwrap:x:+/*/*
      add:x:@../*/http.post/*/payload
         .
            user:x:@auth.ticket.get

// Creating an HTTP POST request towards OpenAI, now decorated.
http.post:"https://api.openai.com/v1/embeddings"
   headers
      Authorization:x:@.token
      Content-Type:application/json
   payload
      input:x:@.arguments/*/prompt
      model:x:@.arguments/*/vector_model
   convert:true

// Sanity checking above invocation
if
   not
      and
         mte:x:@http.post
            .:int:200
         lt:x:@http.post
            .:int:300
   .lambda

      // Oops, error - Logging error and returning status 500 to caller.
      lambda2hyper:x:@http.post
      log.error:Something went wrong while invoking OpenAI
         error:x:@http.post/*/content/*/error/*/message
      response.status.set:x:@http.post
      unwrap:x:+/*
      return
         error:bool:true
         result:x:@http.post/*/content/*/error/*/message

/*
 * Looping through each vector in model, finding the most relevant snippets.
 *
 * Notice, this will calculate the dot product from the question and all training snippets,
 * and use the training snippets with the highest dot product as "the context from which to answer the question".
 *
 * Resulting in that we provide relevant information to OpenAI, allowing them to correctly assemble
 * a completion, based upon the relevant information (context) and the way the question was phrased.
 */
.snippets
.continue:bool:true
.offset:int:0
while:x:@.continue

   // Selecting 50 snippets from type.
   data.read
      table:ml_training_snippets
      columns
         id
         prompt
         completion
         embedding
         uri
      where
         and
            embedding.neq
            type.eq:x:@.arguments/*/type
      limit:50
      offset:x:@.offset
      order:created

   // Looping through all records returned above.
   for-each:x:@data.read/*

      // Calculating dot product of currently iterated snippet and [.vector].
      strings.split:x:@.dp/#/*/embedding
         .:,
      math.dot
         get-nodes:x:@strings.split/*
         get-nodes:x:@http.post/*/content/*/data/0/*/embedding/*

      // Adding result to above [.snippets] collection.
      unwrap:x:+/*/*/*
      add:x:@.snippets
         .
            .
               uri:x:@.dp/#/*/uri
               prompt:x:@.dp/#/*/prompt
               completion:x:@.dp/#/*/completion
               dot:x:@math.dot

   // Sorting [.snippets] such that highest dot product comes first.
   sort:x:@.snippets/*
      if
         mt
            get-value:x:@.lhs/#/*/dot
            get-value:x:@.rhs/#/*/dot
         .lambda
            set-value:x:@.result
               .:int:-1
      else-if
         lt
            get-value:x:@.lhs/#/*/dot
            get-value:x:@.rhs/#/*/dot
         .lambda
            set-value:x:@.result
               .:int:1
      else
         set-value:x:@.result
            .:int:0

   // Updating [.snippets] now with the 50 top most relevant snippets.
   remove-nodes:x:@.snippets/*
   add:x:@.snippets
      get-nodes:x:@sort/*/[0,50]

   // Checking if we're done.
   if
      lt
         get-count:x:@data.read/*
         .:int:50
      .lambda

         // We're done! We've found the top 5 most relevant snippets from our training material.
         set-value:x:@.continue
            .:bool:false
   
   // Incrementing offset.
   math.increment:x:@.offset
      get-count:x:@data.read/*

/*
 * Now we have our most relevant snippets and we can use these to generate an answer
 * to whatever question the caller asked, by doing a little bit of "AI trickery".
 *
 * However, in case user asked a question that's completely irrelevant to the subject
 * of our training snippets, we pass the question to OpenAI using
 * the default model, and just keeping the question "as is".
 */
if
   and
      lt:x:@.snippets/0/*/dot
         convert:x:@.arguments/*/threshold
            type:double
      get-value:x:@.arguments/*/chat
   .lambda

      /*
       * Semantic search resulted in zero high quality snippets,
       * invoking completion with question without context.
       */
      add:x:./*/io.file.execute
         get-nodes:x:@.arguments/*
      io.file.execute:/system/openai/prompt.implementation/completion-prompt.hl
      return-nodes:x:@io.file.execute/*

/*
 * Now we know which snippets are relevant, and we can generate our prompt, by creating a context
 * which we pass on to OpenAI to help it answer the question based upon the snippets matching
 * the question.
 */
.context

// URL of pages used to generate result.
.above

// Checking if we've got a prefix at which point our question changes.
if
   and
      exists:x:@.arguments/*/prefix
      not-null:x:@.arguments/*/prefix
   .lambda

      // Prefixed model, making sure we use prefix.
      set-value:x:@.context
         strings.concat
            get-value:x:@.arguments/*/prefix
            .:"\r\n\r\nQUESTION: "

else

   // No prefix.
   set-value:x:@.context
      .:"Answer following QUESTION with the given CONTEXT.\r\n\r\nQUESTION: "

// Removing prefix since we have manually create it.
set-value:x:@.arguments/*/prefix
   .:

// Creating our "context".
set-value:x:@.context
   strings.concat
      get-value:x:@.context
      get-value:x:@.arguments/*/prompt
      .:"\r\n\r\n"
      .:"CONTEXT: "

/*
 * Figuring out maximum length of context.
 *
 * Maximum length of context is the number of tokens the model can handle,
 * subtracting the number of tokens requested, multiplied by 2 to get byte count.
 */
.max-count
switch:x:@.arguments/*/model

   case:text-davinci-003

      set-value:x:@.max-count
         .:int:4096

   case:text-ada-001
   case:text-curie-001
   case:text-babbage-001

      set-value:x:@.max-count
         .:int:2048

// Subtracting max_tokens from model from [.max-count].
set-value:x:@.max-count
   math.subtract
      get-value:x:@.max-count
      convert:x:@.arguments/*/max_tokens
         type:int

/*
 * Multiplying [.max-count] by 2 as a buffer to ensure we never
 * request more tokens than the model can handle.
 */
set-value:x:@.max-count
   math.multiply
      get-value:x:@.max-count
      .:int:2

// Looping through all snippets to create our context.
for-each:x:@.snippets/*

   /*
    * Making sure our prompt's complete length never exceeds maximum characters,
    * and that we only use relevant snippets above specified threshold from our model.
    */
   if
      and
         mt:x:@.dp/#/*/dot
            convert:x:@.arguments/*/threshold
               type:double
         lt
            math.add
               strings.byte-count:x:@.context
               strings.byte-count:x:@.dp/#/*/prompt
               strings.byte-count:x:@.dp/#/*/completion
            get-value:x:@.max-count
      .lambda

         // We still haven't reached our character threshold yet, and we still have relevant snippets.
         set-value:x:@.context
            strings.concat
               get-value:x:@.context
               .:"\r\n\r\n"
               get-value:x:@.dp/#/*/prompt
               .:"\r\n\r\n"
               get-value:x:@.dp/#/*/completion

         // To support old-style URIs, we need to do some trickery here.
         .uri
         set-value:x:@.uri
            get-value:x:@.dp/#/*/uri
         if
            and
               exists:x:@.dp/#/*/uri
               not-null:x:@.dp/#/*/uri
               strings.contains:x:@.dp/#/*/uri
                  .:"://"
            .lambda

               // URL contains "https://" parts.
               strings.split:x:@.dp/#/*/uri
                  .:"://"
               strings.split:x:@strings.split/0/-
                  .:":"
               set-value:x:@.uri
                  strings.concat
                     get-value:x:@strings.split/@strings.split/0
                     .:"://"
                     get-value:x:@strings.split/0

         // Checking if we need to store URI as reference.
         if
            and
               not-null:x:@.uri
               not-exists:x:@.above/*/*/uri/={@.uri}
            .lambda

               // Adding reference to URL to [.above] collection.
               unwrap:x:+/*/*/*
               add:x:@.above
                  .
                     .
                        uri:x:@.uri
                        prompt:x:@.dp/#/*/prompt

// Making sure OpenAI doesn't return "Answer:" to us
set-value:x:@.context
   strings.concat
      get-value:x:@.context
      .:"\r\nANSWER: "

// Verifying that user actually wants to have a completion/chat response.
if
   or
      not-exists:x:@.arguments/*/chat
      get-value:x:@.arguments/*/chat
   .lambda

      // Invoking default prompt completion.
      add:x:./*/io.file.execute
         get-nodes:x:@.arguments/*
      set-value:x:./*/io.file.execute/*/prompt
         get-value:x:@.context
      io.file.execute:/system/openai/prompt.implementation/completion-prompt.hl
      add:x:../*/return-nodes
         get-nodes:x:@io.file.execute/*

// Making sure we return URIs if caller asked for it.
if
   and
      exists:x:@.arguments/*/references
      get-value:x:@.arguments/*/references
   .lambda
      add:x:+/*/*
         get-nodes:x:@.above/*
      add:x:../*/return-nodes
         .
            references

// Returning result of above invocation.
return-nodes
